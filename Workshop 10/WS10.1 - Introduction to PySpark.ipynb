{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark API and Data Structures\n",
    "\n",
    "Source: https://realpython.com/pyspark-intro/#next-steps-for-real-big-data-processing\n",
    "\n",
    "The entry-point of any PySpark program is a `pyspark.SparkContext` (or simply `SparkContext`) module. It allows us to connect to a Spark cluster and create a specialized data structures called **Resilient Distributed Datasets (RDDs)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2c93e86c14a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m sc = pyspark.SparkContext(\n\u001b[0;32m      3\u001b[0m     \u001b[0mmaster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'local[*]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     appName='muict-ds-pyspark')\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(\n",
    "    master='local[*]',\n",
    "    appName='muict-ds-pyspark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "* `master`: The URL of the cluster it connects to.\n",
    "* `appName`: Name of your job.\n",
    "\n",
    "In this example, we use `local[*]` for `master` which is a special string denoting that we want to use a *local* cluster or a *single-machine mode*. The `*` tells Spark to create as many worker threads as logical cores on your machine. Also we can, for instance, use `local` or `local[2]` to only use single or two cores on the machine, respectively.\n",
    "\n",
    "**Note**: Setting up a Spark cluster is outside the scope of this course. Typically, we need a data engineering team to handle such setup and installation, or we pay for the cluster service from cloud computing providers (e.g., Amazon AWS, Google Cloud, Microsoft Azure, etc.). You can read more information about the cluster setup from this link (https://phoenixnap.com/kb/install-spark-on-ubuntu , https://stackoverflow.com/questions/35835649/reading-a-file-in-hdfs-from-pyspark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `SparkContext` should be created ONLY ONCE per session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RDDs\n",
    "\n",
    "There are two ways that we can create a RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized Collections\n",
    "The **first approach** is to take an existing collection in our program and pass it to `SparkContext`'s `parallelize` method. All elements in the collection will then be copied to form a *distributed dataset* that can be operated on *in parallel*. This approach is good when you would like to experiment with some commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = list(range(1,6))\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "print(numbers_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However it is **NOT** typically used in practice because this approach assumes that the size of the entire dataset can be loaded into a single computer memory, which is NOT the case when we are dealing with big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Datasets\n",
    "\n",
    "The **second approach**, which is a common way, is to create RDDs from any external storage supported by Hadoop, including local file system, HDFS, Cassandra, HBase, Amazon S3, etc.\n",
    "\n",
    "Text file RDDs can be created using `textFile` method on `SparkContext`. The `textFile` method reads the file into a RDD with each line in the file being an element in the RDD collection. The exeternal storage can be the local file system.\n",
    "\n",
    "Let's see how to import a text file, named `big-data-wikipedia.txt` from our local disk into the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = sc.textFile('file:////home/jovyan/pyspark-data/big-data-wikipedia.txt')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More realistically, the external storage is usually a distributed file system such as *Amazon S3* (i.e., s3a://) or *HDFS* (i.e., hdfs://). There are other data sources which can be integrated with Spark and used to create RDDs including JDBC, Cassandra, Elasticsearch, etc.\n",
    "\n",
    "In this course, we will only focus on the *local disk* for the external storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we do with RDDs?\n",
    "\n",
    "**Resilient Distributed Datasets (RDDs)** hide all the complexity of transforming and distributing your data, and parallelizing the operations you perform on them *automatically across multiple nodes* by a scheduler if youâ€™re running on a cluster.\n",
    "\n",
    "In Spark, all works are expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result.\n",
    "\n",
    "RDDs support two types of operations:\n",
    "1. **Transformations**: create a new dataset from an existing one, and\n",
    "2. **Actions**: return a value to the driver program after running a computation on the dataset\n",
    "\n",
    "The following sections are examples of the transformations and actions. The full list of available operations can be found here (https://spark.apache.org/docs/latest/api/python/reference/pyspark.html).\n",
    "\n",
    "Source:\n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "* https://www.youtube.com/playlist?list=PLot-YkcC7wZ_2sxmRTZr2c121rjcaleqv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation\n",
    "\n",
    "Transformations are operations on RDDs which will return a new RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter() transformation\n",
    "\n",
    "The `filter` function returns a new RDD with a subset of the data in the original RDD. We define the selection criteria using a *lambda* function where it only selects the items that the lambda function return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of lines that contains a `big` word.\n",
    "big_lines = txt.filter(lambda line: 'big' in line.lower())\n",
    "print(big_lines)\n",
    "print(big_lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map() transformation\n",
    "\n",
    "The `map` function applies a function to each item in the input RDD and return the new value of each element. The *return type* of the map function is not necessary the same as its *input type*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of characters in each line\n",
    "lens = txt.map(lambda line: len(line))\n",
    "print(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first and the last words, split by ' ', from each line\n",
    "def first_last(line):\n",
    "    words = line.split(' ')\n",
    "    return f'{words[0]} {words[-1]}'\n",
    "\n",
    "first_last_words = txt.map(first_last)\n",
    "print(first_last_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the output RDD as text files to a directory via the `saveAsTextFile` function. Note that we will discuss more about the `saveAsTextFile` later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if the directory alreay exists, it will show an error\n",
    "if os.path.isdir('out.lengths'):\n",
    "    shutil.rmtree('out.lengths')\n",
    "lens.saveAsTextFile('out.lengths')\n",
    "if os.path.isdir('out.first_last'):\n",
    "    shutil.rmtree('out.first_last')\n",
    "first_last_words.saveAsTextFile('out.first_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you open the `out.lengths` or the `out.first_last` directory, the output(s) are stored in the `part-00001`, `part-00002`, etc. depending on the number of cores we specified in SparkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatMap() transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `flatMap` function is similar to the `map` function, except that the results are *flatten* before being returned.\n",
    "\n",
    "![flatMap vs map](flatMap_vs_map.png)\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=xhBnHBvOHwc&list=PLot-YkcC7wZ_2sxmRTZr2c121rjcaleqv&index=9\n",
    "\n",
    "Here is an example when we apply the `split` to each line using `map` and `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_out = txt.map(lambda line: line.split(' '))\n",
    "flatmap_out = txt.flatMap(lambda line: line.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `first` function to get the first element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_out.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flatmap_out.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the `map` function returns a list of words of the first line, while the `flatMap` function returns the first word of the first line. This is because after the list of the lists of words are flatten it becomes a single list containing a sequence of all words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample() transformation\n",
    "\n",
    "The `sample` operation creates a random sample from an RDD. This is useful for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = txt.sample(withReplacement=False, fraction=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distinct() operation\n",
    "\n",
    "The distinct operation return a new RDD containing the distinct elements of this RDD.\n",
    "\n",
    "Note: This transformation is **expensive** because it requires shuffling all the data across partitions to ensure that we receive only one copy of each element\n",
    "\n",
    "Try to avoid this oepration if it is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc.parallelize([1,1,2,2,2,2,3,4,1]).distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collect` function is used to retrieve all the elements of the dataset (from all nodes) to the driver node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set operation\n",
    "\n",
    "There are several *Set* operations which are performed on two RDDs and produce one resulting RDD.\n",
    "* union\n",
    "* intersection\n",
    "* subtract\n",
    "* catesian product\n",
    "\n",
    "Here is an example of the `intersection` operation. This operation removes all duplicates including the duplicates from single RDD before returning the results. This is also an expensive operation since it requires shuffling all the data across partitions to identify common elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,1,2,2,2,2,3,4,1])\n",
    "rdd2 = sc.parallelize([2,2,3,4,5,6,6,7])\n",
    "intersect_rdd = rdd1.intersection(rdd2)\n",
    "print(intersect_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action\n",
    "\n",
    "Actions are the operations which will return a result to the driver node or write the result to an external storage system, and kick off a computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect() action\n",
    "\n",
    "The `collect` function is used to retrieve the entire RDD (from all nodes) and returns it to the driver node in the form of a regular collection or value.\n",
    "\n",
    "**Important**: We should use the `collect()` on smaller dataset usually after `filter()`, `group()`, `count()` etc. Retrieving larger dataset results in out of memory.\n",
    "\n",
    "It is widely used in unit tests, to compare the value of our RDD with our expected result, as long as the entire contents of the resulting RDD can fit in memory of the driver node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in sample_txt.collect():\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count() action\n",
    "\n",
    "The `count` function returns the number of elements (or rows) in an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## countByValue() action\n",
    "\n",
    "The `countByValue` function determines unique item values and returns a map of each unique value to its count. This function is useful when the RDD contains duplicate rows, and you want to count how many of each unique row values you have.\n",
    "\n",
    "Here is an example of how to count the number of wards in the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line by a space ' ' and flatten the resulting RDD\n",
    "words = txt.flatMap(lambda line: line.split(' '))\n",
    "\n",
    "# Count the number of each word\n",
    "# The function `countByValue` returns a defaultdict\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take() action\n",
    "\n",
    "The `take` function takes `n` elements from an RDD. This is useful if you would like to take a peek at the RDD for unit tests and quick debugging.\n",
    "\n",
    "**Caution**: The `take` function will try to reduce the number of partitions it accesses. Thus, it is possible that we will get a biased collection, and it does not necessary return the elements in the order we might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words.take(6):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saveAsTextFile() action\n",
    "\n",
    "The `saveAsTextFile` function can be used to write data out to a distributed storage system such as HDFS or Amazom S3, or even local file system.\n",
    "\n",
    "The following is the code similar to the previous example above that saves two RDD to the local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if the directory alreay exists, it will show an error\n",
    "if os.path.isdir('out.lengths'):\n",
    "    shutil.rmtree('out.lengths')\n",
    "lens.saveAsTextFile('out.lengths')\n",
    "if os.path.isdir('out.first_last'):\n",
    "    shutil.rmtree('out.first_last')\n",
    "first_last_words.saveAsTextFile('out.first_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce() action\n",
    "\n",
    "The `reduce` function takes a function that operates on *two* elements of the type in the input RDD and returns *a new element* of the *same type*. This function produces the same results when repetitively applied on the same set of RDD data, and reduces to a single value.\n",
    "\n",
    "With `reduce` operation, we can perform different types of aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([2,5,6,7,1])\n",
    "product = numbers.reduce(lambda x, y: x*y)\n",
    "# 2,    5,    6,    7,    1\n",
    "# 2*5=10 ,    6*7=42  ,    1\n",
    "# 10     ,    42*1=42\n",
    "# 10*42=420\n",
    "print(f'prod={product}')\n",
    "\n",
    "summation = numbers.reduce(lambda x, y: x+y)\n",
    "print(f'sum={summation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduceByKey() action\n",
    "\n",
    "The `reduceByKey` function is similar to the `reduce` action, but it first merges the values for each key before using an associative and commutative reduce function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = txt.flatMap(lambda line: line.split(' '))\n",
    "\n",
    "# Convert to (key, value) pairs\n",
    "kv_words = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Merge the values for each key using a lambda function.\n",
    "wordCounts = kv_words.reduceByKey(lambda x, y: x+y)\n",
    "for word, count in sorted(wordCounts.collect()):\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations vs Actions\n",
    "\n",
    "All transformations in Spark are *lazy*, in that they DO NOT compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. *This design enables Spark to run more efficiently*. \n",
    "\n",
    "For example, we can realize that a dataset created through `map` will be used in a `reduce` and return only the result of the `reduce` to the driver, rather than the larger mapped dataset.\n",
    "\n",
    "<!-- Transformations on RDDs are **lazily evaluated**, meaning that Spark will not begin to execute until it sees an action.\n",
    "\n",
    "Rather than thinking of an RDD as containing a specific data, think of each RDD as consisting of **sequence of instructions** on how to compute the data that we build up through transformations.\n",
    "\n",
    "Spark uses laze evaluation to **reduce the number of passes** it has to take over our data by grouping operations together. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching and persistence\n",
    "\n",
    "Sometimes we would like to call actions on the same RDD multiple times.\n",
    "\n",
    "If we do this naively, RDDs and all of its dependencies are **recomputed** each time an action is called on the RDD. This can be very expensive, especially for some iterative algorithms, which would call actions on the same dataset many times.\n",
    "\n",
    "If we want to reuse an RDD in multiple actions, we can ask Spark to persist by calling the `persist()` function on the RDD. When we persist an RDD, the first time it is computed in an action, it will be kept in memory across the nodes.\n",
    "\n",
    "This allows the future action to be much faster. Caching is a critical mechanism for an iterative algorithm or a fast interactive use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([2,5,6,7,1])\n",
    "numbers.persist(pyspark.StorageLevel.MEMORY_ONLY)  # =numbers.cache()\n",
    "\n",
    "# At this point, the parallelized transformation will executed\n",
    "# to distribute the RDD from the driver program to all worker threads\n",
    "# and call `reduce` on this partition.\n",
    "# Since this RDD is persisted, so it will be kept in memory across\n",
    "# the worker threads.\n",
    "product = numbers.reduce(lambda x, y: x*y)\n",
    "print(f'prod={product}')\n",
    "\n",
    "# At this point, Spark will NOT parallelize the transformations again.\n",
    "# Instead, it can immediately count the number of elements.\n",
    "count = numbers.count()\n",
    "print(f'count={count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `RDD.cache() = RDD.persist(MEMORY_ONLY)`.\n",
    "\n",
    "Spark's sotrage levels are meant to provide different trade-offs between memory usage and CPU efficiency.\n",
    "* If RDD can fit in memory, use `MEMORY_ONLY`. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible\n",
    "* If not, try to use `MEMORY_ONLY_SER` for more space-efficient, but more CPU intensive to read\n",
    "* DO NOT save to disk (e.g., `MEMORY_AND_DISK`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`) unless the functions that computed your datasets are expensive, or they filter a significant amount of the data. Otherwise, recomputing the partitions may be as fast as we read it from the disk.\n",
    "\n",
    "Spark will *evict old partitions automatically* using a *Least Recently Used* cache policy.\n",
    "\n",
    "**Caution**: caching unnecessary data can cause Spark to evict useful data and lead to longer re-computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
