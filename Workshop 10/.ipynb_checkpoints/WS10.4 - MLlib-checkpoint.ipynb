{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare DataFrame via Spark SQL\n",
    "\n",
    "As the MLlib is the `DataFrame`-based machine learning APIs, we need to load and prepare datasets into `DataFrame`. Here we will be using Spark SQL to load the data.\n",
    "\n",
    "You can find the list of all Spark SQL features [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setMaster('local[*]')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Python Spark SQL basic example') \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "    # .config('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the dataset from Kaggle: [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into Spark's DataFrame\n",
    "df = spark.read.csv(\n",
    "    'creditcard.csv',\n",
    "    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of examples\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe via pandas.DataFrame\n",
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe all variables\n",
    "df.describe('V1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column\n",
    "clean_df = df.drop('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns\n",
    "feature_columns = clean_df.drop('Class').columns\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to float and integer\n",
    "for c in feature_columns:\n",
    "    clean_df = clean_df.withColumn(c, clean_df[c].cast('float'))\n",
    "clean_df = clean_df.withColumn('Class', clean_df['Class'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate the features we will use to make predictions into a single column. We can use `VectorAssembler` which is a feature transformer that merges multiple columns into **a vector column**. We have to merge all feature columns into a single vector column, such that it can be used by the MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns, \n",
    "    outputCol='features')\n",
    "feature_df = assembler.transform(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 80% / 10% / 10%\n",
    "train_df, valid_df, test_df = feature_df.randomSplit([0.8,0.1,0.1], seed=42)\n",
    "train_df.groupBy('Class').count().show()\n",
    "valid_df.groupBy('Class').count().show()\n",
    "test_df.groupBy('Class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - Machine Learn Library (MLlib)\n",
    "\n",
    "MLlib is Sparkâ€™s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "- **ML Algorithms**: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- **Featurization**: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- **Pipelines**: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- **Persistence**: saving and load algorithms, models, and Pipelines\n",
    "- **Utilities**: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "Ref: \n",
    "* https://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    labelCol='Class', \n",
    "    featuresCol='features')\n",
    "model = model.fit(train_df)  # fit() returns DecisionTreeClassificationModel\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "# dt = DecisionTreeClassifier(\n",
    "#     labelCol='Class', \n",
    "#     featuresCol='features')\n",
    "# pipeline = Pipeline(stages=[dt])\n",
    "# model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DecisionTreeClassificationModel\n",
    "save_path = 'dt.model'\n",
    "if os.path.isdir(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "model.save('dt.model')\n",
    "\n",
    "# # Save PipelineModel\n",
    "# save_path = 'pipeline.model'\n",
    "# if os.path.isdir(save_path):\n",
    "#     shutil.rmtree(save_path)\n",
    "# model.save('pipeline.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DecisionTreeClassificationModel\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "load_model2 = DecisionTreeClassificationModel.load('dt.model')\n",
    "\n",
    "# # Load PipelineModel\n",
    "# from pyspark.ml import PipelineModel\n",
    "# load_model = PipelineModel.load('pipeline.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_valid_df = model.transform(valid_df)\n",
    "\n",
    "# Make predictions.\n",
    "pred_test_df = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid_df.select('prediction','Class').show()\n",
    "pred_test_df.select('prediction','Class').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Accuracy\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol='Class', \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "valid_acc = acc_eval.evaluate(pred_valid_df)\n",
    "test_acc = acc_eval.evaluate(pred_test_df)\n",
    "print(f'Valid acc: {valid_acc}')\n",
    "print(f'Test acc: {test_acc}')\n",
    "\n",
    "# Area under ROC\n",
    "roc_eval = BinaryClassificationEvaluator(\n",
    "    labelCol='Class',\n",
    "    rawPredictionCol='rawPrediction', \n",
    "    metricName='areaUnderROC')\n",
    "valid_roc = roc_eval.evaluate(pred_valid_df)\n",
    "test_roc = roc_eval.evaluate(pred_test_df)\n",
    "print(f'Valid ROC: {valid_roc}')\n",
    "print(f'Test ROC: {test_roc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Other ML Libraries\n",
    "\n",
    "Typically the progress of incorporating the state-of-the-art ML techniques in Spark cannot catch up with the progress of ML research in time. This makes the process of prototyping ML models with Spark becomes time-consuming as we need to re-implement the recent ML techniques to be able to work with Spark.\n",
    "\n",
    "One potential solution to solve this problem is to utilize the Spark to handle data storage and data pre-processing part, and then load the smaller dataset into the local machine for prototyping with the state-of-the-art ML libraries such as scikit-learn, Tensorflow, etc. Once we know which ones would work, then we can spend time only implement a few techniques with the real big data in Spark.\n",
    "\n",
    "The smaller dataset can be obtained via random sampling, selecting the recent data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_small_df = clean_df.sample(withReplacement=False, fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without replacement by default\n",
    "stratifed_small_df = clean_df.sampleBy('Class', fractions={0: 0.2, 1: 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.groupBy('Class').count().show()\n",
    "random_small_df.groupBy('Class').count().show()\n",
    "stratifed_small_df.groupBy('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf = stratifed_small_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pddf.drop(columns=['Class']).values.astype(np.float32)\n",
    "y = pddf['Class'].values.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: scikit-learn\n",
    "\n",
    "Once we load the data into pandas.DataFrame, we can then use the similar code from WS6.1 from Lecture 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    random_state=42,\n",
    "    test_size=0.20)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, y_test,\n",
    "    random_state=42,\n",
    "    test_size=0.50)\n",
    "\n",
    "print(f'Training set: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation set: {X_valid.shape}, {y_valid.shape}')\n",
    "print(f'Test set: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=50, \n",
    "    max_depth=50,\n",
    "    min_samples_leaf=10)\n",
    "\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_valid = model.predict(X_valid)\n",
    "y_hat_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('Training Set')\n",
    "print(confusion_matrix(y_true=y_train, y_pred=y_hat_train))\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_train, y_pred=y_hat_train):.2f}')\n",
    "print(f'F1-score: {f1_score(y_true=y_train, y_pred=y_hat_train):.2f}')\n",
    "print('')\n",
    "print('Validation Set')\n",
    "print(confusion_matrix(y_true=y_valid, y_pred=y_hat_valid))\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_valid, y_pred=y_hat_valid):.2f}')\n",
    "print(f'F1-score: {f1_score(y_true=y_valid, y_pred=y_hat_valid):.2f}')\n",
    "print('')\n",
    "print('Test Set')\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_hat_test))\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_test, y_pred=y_hat_test):.2f}')\n",
    "print(f'F1-score: {f1_score(y_true=y_test, y_pred=y_hat_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
