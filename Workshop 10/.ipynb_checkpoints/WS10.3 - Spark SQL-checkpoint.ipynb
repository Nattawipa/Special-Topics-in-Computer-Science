{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkSession` is an entry point to programming Spark with the Dataset and DataFrame API. As of Spark 2.0, `SparkContext` is replaced by `SparkSession`. However, we are keeping the class here for backward compatibility.\n",
    "\n",
    "To create a basic `SparkSession`, just use `SparkSession.builder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local[*]')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Python Spark SQL basic example') \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "    # .config('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create the `SparkSession` from the existing `SparkContext`.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('Python Spark SQL basic example') \\\n",
    "    .setMaster('local[2]')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "```\n",
    "\n",
    "We will cover only the basic command in this course as the Spark SQL will be mainly used to load the CSV file into the Spark DataFrame. Such DataFrame will be further used by the Spark MLlib in the next workshop. You can find the list of all features [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "\n",
    "Spark SQL supports operating on a variety of data sources through the `DataFrame` interface. A `DataFrame` can be operated on using relational transformations and can also be used to create a temporary view. Registering a `DataFrame` as a temporary view allows you to run SQL queries over its data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "Spark SQL includes a data source that can read data from other databases using JDBC. The results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. See more from https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "\n",
    "Here are some examples of how to read the data from SQL Server and PostgreSQL:\n",
    "- SQL server\n",
    "https://kontext.tech/column/spark/290/connect-to-sql-server-in-spark-pyspark\n",
    "- PostgreSQL\n",
    "https://stackoverflow.com/questions/34948296/using-pyspark-to-connect-to-postgresql\n",
    "\n",
    "For the sake of simplicity, we will only cover the read data from a CSV file. It should be noted that the techniques we cover here can be applied to any data sources as long as they can be loaded into `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local CSV files\n",
    "\n",
    "We will use the `titanic_data_cleaner.csv` file from the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into Spark\n",
    "df_csv = spark.read.csv(\n",
    "    'titanic_data_cleaner.csv',\n",
    "    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show some rows\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "df_csv.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can return the contents of the Spark `DataFrame` as Pandas `pandas.DataFrame` via `toPandas()`.\n",
    "\n",
    "**Caution**: The conversion takes time! Make sure you run on a small DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the contents of this DataFrame as Pandas pandas.DataFrame\n",
    "# Cautions: the conversion takes time! --> make sure you run on \n",
    "df_csv.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select` projects a set of expressions and returns a new `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 'Sex' column\n",
    "df_csv.select('Sex').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 'SibSp' column and increase the 'SibSp' column by 1\n",
    "df_csv.select(df_csv['SibSp'], df_csv['SibSp']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the passengers who have SibSp>0\n",
    "df_csv.filter(df_csv['SibSp'] > 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per passenger class\n",
    "df_csv.groupBy('Pclass').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of survived and died passengers\n",
    "df_csv.groupBy('Survived').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sql` function on a `SparkSession` enables applications to run SQL queries programmatically and returns the result as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a table\n",
    "df_csv.registerTempTable('titanic')\n",
    "\n",
    "# Execute SQL query\n",
    "df_sql = spark.sql('SELECT Age, Pclass, Fare, Survived FROM titanic')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in functions\n",
    "\n",
    "Ref: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floor and ceil\n",
    "df_csv.select(\n",
    "    df_csv['Fare'],\n",
    "    sql_fn.floor(df_csv['Fare']),\n",
    "    sql_fn.ceil(df_csv['Fare'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate on the entire DataFrame without groups\n",
    "df_csv.agg(\n",
    "    sql_fn.count(df_csv['Fare']).alias('count_fare'),\n",
    "    sql_fn.countDistinct(df_csv['Fare']),\n",
    "    sql_fn.mean(df_csv['Fare']),\n",
    "    sql_fn.sum(df_csv['Fare']),\n",
    "    sql_fn.corr(df_csv['Fare'], df_csv['PClass'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate on the grouped data\n",
    "df_csv.groupBy('Survived').agg(\n",
    "    sql_fn.mean(df_csv['Fare']),\n",
    "    sql_fn.sum(df_csv['Fare'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
