{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq0wJCXgXswZ"
   },
   "source": [
    "# 0. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "abkXlwg4WvtZ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3aeef50c6911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKxNq3ZpXznp"
   },
   "outputs": [],
   "source": [
    "# Check the directory\n",
    "!ls \"/content/gdrive/My Drive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOyi3zWWYK-J"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "data_dir = '/content/gdrive/My Drive/Colab Notebooks/data'\n",
    "\n",
    "!ls '$data_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YT7Cqj2Xg1E"
   },
   "source": [
    "# 1. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ASPeOdcYXoBj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WY2TCkeYdw_"
   },
   "source": [
    "# 2. Load Dataset\n",
    "\n",
    "In this task, you will predict whether a passenger would survive\n",
    "[[kaggle](https://www.kaggle.com/c/titanic)].\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "| Variable  |        Definition                                    |                 Key                               |\n",
    "|-----------------|--------------------------------------------|------------------------------------------------|\n",
    "| survival        | Survival (label)                                  | 0 = No, 1 = Yes                                |\n",
    "| pclass          | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n",
    "| sex             | Sex                                        |                                                |\n",
    "| Age             | Age in years                               |                                                |\n",
    "| sibsp           | # of siblings / spouses aboard the Titanic |                                                |\n",
    "| parch           | # of parents / children aboard the Titanic |                                                |\n",
    "| ticket          | Ticket number                              |                                                |\n",
    "| fare            | Passenger fare                             |                                                |\n",
    "| cabin           | Cabin number                               |                                                |\n",
    "| embarked        | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "We have already done some preprocessing on the data to simplify the next process. You can also try on the raw data.\n",
    "\n",
    "First, you download the [titanic_data_cleaner.csv](https://drive.google.com/file/d/1TSXkI-2yMiWBHx3ylFVlZlW_Clvg9Dah/view?usp=sharing) and then upload it to your Google Drive. The recommended location is in the `Colab Notebooks/data` folder.\n",
    "\n",
    "Then run the following command to read the csv file in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "st0vcx_-YeqS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b2794c013e8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'titanic_data_cleaner.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(data_dir, 'titanic_data_cleaner.csv')\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TefsYPGJYvL5"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF0TgRI1a3DH"
   },
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "In this section, we will prepare the dataset into a format that can be used to train models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoT368WkY3cr"
   },
   "source": [
    "## 3.1 Feature Selection\n",
    "\n",
    "How do we know which features can be used to predict whether the passenger will survided the crash?\n",
    "\n",
    "* Domain Expert Knowledge\n",
    "* Visual Inspection\n",
    "* Feature Selection Algorithms (see more [link1](https://scikit-learn.org/stable/modules/feature_selection.html), [link2](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQPPn0v0ZxZH"
   },
   "outputs": [],
   "source": [
    "# Let's see the Pclass\n",
    "sns.catplot(\n",
    "    data=df, hue='Survived',\n",
    "    x='Pclass', kind='count', palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0HxZsOunsg2"
   },
   "source": [
    "How about the `'Sex'`, `'Embarked'` and `'Cabin'`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DspRRnIXaS-6"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "sns.catplot(\n",
    "    data=df, hue='Survived',\n",
    "    x='Sex', kind='count', palette='Set1')\n",
    "\n",
    "sns.catplot(\n",
    "    data=df, hue='Survived',\n",
    "    x='Embarked', kind='count', palette='Set1')\n",
    "\n",
    "sns.catplot(\n",
    "    data=df, hue='Survived',\n",
    "    x='Cabin', kind='count', palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxVOGRaROKy9"
   },
   "source": [
    "Which feature should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTJMRjYVYxV4"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "data_df = df[['Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiF0x41QcE9i"
   },
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2m6lKmEdYX-"
   },
   "source": [
    "## 3.2 Categorical Columns\n",
    "\n",
    "scikit-learn expects numerical tensors, so we have to convert our `str` data into number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmltQ7-oavAJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sex_enc = LabelEncoder()\n",
    "\n",
    "data_df['Sex_code'] = sex_enc.fit_transform(data_df['Sex'])\n",
    "print(sex_enc.classes_)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5W6hzz72bUa0"
   },
   "outputs": [],
   "source": [
    "emb_df = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wROoNKRzcP2X"
   },
   "outputs": [],
   "source": [
    "clean_df = pd.concat([data_df, emb_df], axis=1)\n",
    "clean_df = clean_df.drop(columns=['Sex','Embarked'])\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewBMT3RRhquJ"
   },
   "source": [
    "# 4. Prepare Train/Valid/Test Sets\n",
    "\n",
    "We first extract the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2ToBgb8i_ny"
   },
   "outputs": [],
   "source": [
    "X = clean_df.drop(columns=['Survived']).values\n",
    "y = clean_df['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3arWoVGjFkx"
   },
   "source": [
    "Next we split the dataset into training/validation/test set.\n",
    "* Training set: `X_train`, `y_train`\n",
    "* Validation set: `X_valid`, `y_valid`\n",
    "* Test set: `X_test`, `y_test`\n",
    "\n",
    "The following is an example of how to split the dataset into a training and a test sets.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    random_state=42,\n",
    "    test_size=0.20)  # 80:20\n",
    "```\n",
    "\n",
    "Let's split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZsvCcKtcxy3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    random_state=42,\n",
    "    test_size=0.20)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, y_test,\n",
    "    random_state=42,\n",
    "    test_size=0.50)\n",
    "\n",
    "print(f'Training set: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation set: {X_valid.shape}, {y_valid.shape}')\n",
    "print(f'Test set: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlznPTX0i7Fx"
   },
   "source": [
    "# 5. Model Selection\n",
    "\n",
    "In this section, you will write code to train the model on the training set and evaluate it on the validation set.\n",
    "\n",
    "Let's start by creating and training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igH9f3vzi8q3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model = DecisionTreeClassifier()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "# scaler.fit(X_train)\n",
    "# s_X_train = scaler.transform(X_train)\n",
    "# model = LogisticRegression()\n",
    "# model = model.fit(s_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY5qwUQRkCYd"
   },
   "source": [
    "Next, we use the trained model to predict whether the passengers in both the training and the validation sets will survive the titanic crash or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIuS-PdjkrT9"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_valid = model.predict(X_valid)\n",
    "\n",
    "# y_hat_train = model.predict(s_X_train)\n",
    "# s_X_valid = scaler.transform(X_valid)\n",
    "# y_hat_valid = model.predict(s_X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjWHE7HskQ7K"
   },
   "source": [
    "Then we determine the prediction performance on the training and the validation set to investigate whether our model has the **underfitting** or **overfitting** problems or not.\n",
    "\n",
    "Here, we use the common metrics for classification problems which are: **accuracy, precision, recall and f1-score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_cpXHeCo7kF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('Training Set')\n",
    "print(confusion_matrix(y_true=y_train, y_pred=y_hat_train))\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_train, y_pred=y_hat_train):.2f}')\n",
    "# print(f'Precision: {precision_score(y_true=y_train, y_pred=y_hat_train):.2f}')\n",
    "# print(f'Recall: {recall_score(y_true=y_train, y_pred=y_hat_train):.2f}')\n",
    "print(f'F1-score: {f1_score(y_true=y_train, y_pred=y_hat_train):.2f}')\n",
    "print('')\n",
    "print('Validation Set')\n",
    "print(confusion_matrix(y_true=y_valid, y_pred=y_hat_valid))\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_valid, y_pred=y_hat_valid):.2f}')\n",
    "# print(f'Precision: {precision_score(y_true=y_valid, y_pred=y_hat_valid):.2f}')\n",
    "# print(f'Recall: {recall_score(y_true=y_valid, y_pred=y_hat_valid):.2f}')\n",
    "print(f'F1-score: {f1_score(y_true=y_valid, y_pred=y_hat_valid):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTzYCys7oUR7"
   },
   "source": [
    "# 6. Evaluation on Test Set\n",
    "\n",
    "Once we found a best model, we then evaluate the trained model with the test set to estimate the performance on the **unseen** examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdinRZgmr1fD"
   },
   "outputs": [],
   "source": [
    "y_hat_test = model.predict(X_test)\n",
    "\n",
    "# s_X_test = scaler.transform(X_test)\n",
    "# y_hat_test = model.predict(s_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBvfc5AFr640"
   },
   "outputs": [],
   "source": [
    "print('Test Set')\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_test, y_pred=y_hat_test):.2f}')\n",
    "# print(f'Precision: {precision_score(y_true=y_test, y_pred=y_hat_test):.2f}')\n",
    "# print(f'Recall: {recall_score(y_true=y_test, y_pred=y_hat_test):.2f}')\n",
    "print(f'F1-score: {f1_score(y_true=y_test, y_pred=y_hat_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy47pptk6wF8"
   },
   "source": [
    "# 7. Try Other Classifiers\n",
    "\n",
    "There are a large number of supervised-ML algorithms that you can use. Please try other classifiers below and try to achieve the best performance on the test set.\n",
    "\n",
    "* [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html): try to change `C` and `penalty`.\n",
    "* [C-Support Vector Classification](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): try to change `C`, `gamma`.\n",
    "* [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html): try to change `n_estimators`, `max_depth`, `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkugeGdybhXe"
   },
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Please note that **feature scaling** is essential for machine learning algorithms that calculate distances between data (except DecisionTree, RandomForest, XGBoost, etc.). This is to prevent the features with a larger range dominate the smaller ones. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "Scikit-learn provides several functions that we can use to scale the range of all features to be in the similar range.\n",
    "\n",
    "* [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "* [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "* [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)\n",
    "\n",
    "For example,\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "X = [[ 1., -2.,  2.],\n",
    "     [ -2.,  1.,  3.],\n",
    "     [ 4.,  1., -2.]]\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X)\n",
    " \n",
    "scaled_X = scaler.transform(X)\n",
    " \n",
    "print(scaled_X)\n",
    "print(f'Mean: {np.mean(scaled_X, axis=0)}')\n",
    "print(f'Std: {np.std(scaled_X, axis=0)}')\n",
    "\n",
    "# Output\n",
    "# [[ 0.  -2.   0. ]\n",
    "#  [-1.   0.   0.4]\n",
    "#  [ 1.   0.  -1.6]]\n",
    "# Mean: [ 0.         -0.66666667 -0.4       ]\n",
    "# Std: [0.81649658 0.94280904 0.86409876]\n",
    "```\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "The scaler should be **fitted (i.e.,** `scaler.fit(X)`) **to the training set only**. This is to prevent the scaler from observing unseen data. The fitted scaler is then used to transform (i.e., `scaler.transform(X)`) the validation and the test sets before the prediction.\n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. Try the other classifiers as specified above.\n",
    "\n",
    "1. Modify Step 5 and 6 to include the feature selection, except when you are using the DecisionTree or the RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUlf9U8ur_wq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "W6.1 - Model Selection for Classification (solution).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
