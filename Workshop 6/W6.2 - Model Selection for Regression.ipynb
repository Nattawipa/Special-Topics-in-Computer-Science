{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq0wJCXgXswZ"
   },
   "source": [
    "# 0. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abkXlwg4WvtZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKxNq3ZpXznp"
   },
   "outputs": [],
   "source": [
    "# Check the directory\n",
    "!ls \"/content/gdrive/My Drive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOyi3zWWYK-J"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "data_dir = '/content/gdrive/My Drive/Colab Notebooks/data'\n",
    "\n",
    "!ls '$data_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YT7Cqj2Xg1E"
   },
   "source": [
    "# 1. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASPeOdcYXoBj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WY2TCkeYdw_"
   },
   "source": [
    "# 2. Load Dataset\n",
    "\n",
    "In this task, you will predict the GDP of each countries\n",
    "[[kaggle](https://www.kaggle.com/fernandol/countries-of-the-world)].\n",
    "\n",
    "We have already done some preprocessing on the data to simplify the next process. You can also try on the raw data. We also recommend you to check the data visualization technique in this [notebook](https://www.kaggle.com/mehmettek/data-science-with-world-countries).\n",
    "\n",
    "First, you download the [countries of the world_cleaner.csv](https://drive.google.com/file/d/1KXS-9AOsc1a9OG9r44EnJpHA-GCJuGPL/view?usp=sharing) and then upload it to your Google Drive. The recommended location is in the `Colab Notebooks/data` folder.\n",
    "\n",
    "Then run the following command to read the csv file in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "st0vcx_-YeqS"
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(data_dir, 'countries of the world_cleaner.csv')\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TefsYPGJYvL5"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF0TgRI1a3DH"
   },
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "In this section, we will prepare the dataset into a format that can be used to train models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoT368WkY3cr"
   },
   "source": [
    "## 3.1 Feature Selection\n",
    "\n",
    "How do we know which features can be used to predict whether the passenger will survided the crash?\n",
    "\n",
    "* Domain Expert Knowledge\n",
    "* Visual Inspection\n",
    "* Feature Selection Algorithms (see more [link1](https://scikit-learn.org/stable/modules/feature_selection.html), [link2](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiF0x41QcE9i"
   },
   "outputs": [],
   "source": [
    "# Drop unused features\n",
    "data_df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWbzlpwwtt5k"
   },
   "source": [
    "## 3.2 Deal with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uC__CL2ItyBl"
   },
   "outputs": [],
   "source": [
    "# Investigate NaN in DataFrame\n",
    "total = data_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data_df.isnull().sum()/data_df.isnull().count())\n",
    "percent = percent.sort_values(ascending=False)\n",
    "missing_data = pd.concat(\n",
    "    [total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print('Missing data:')\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXrhI_2iuW9M"
   },
   "outputs": [],
   "source": [
    "# Determine the values to be replaced for NaN\n",
    "replace_values = {}\n",
    "for column in data_df.columns:\n",
    "  if data_df[column].isnull().any():\n",
    "    if data_df[column].dtype == np.float64:\n",
    "      # Use mean for float\n",
    "      replace_values[column] = data_df[column].mean()\n",
    "    elif data_df[column].dtype == type(object):\n",
    "      # Use 'UNK' keyword for string\n",
    "      replace_values[column] = 'UNK'\n",
    "print(replace_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiNEekzbugdH"
   },
   "outputs": [],
   "source": [
    "# Replace NaN values according to the `replace_values` dictionary\n",
    "data_df = data_df.fillna(value=replace_values)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNwC95A3useE"
   },
   "outputs": [],
   "source": [
    "# Investigate NaN in DataFrame\n",
    "total = data_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data_df.isnull().sum()/data_df.isnull().count())\n",
    "percent = percent.sort_values(ascending=False)\n",
    "missing_data = pd.concat(\n",
    "    [total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print('Missing data:')\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2m6lKmEdYX-"
   },
   "source": [
    "## 3.3 Categorical Columns\n",
    "\n",
    "Scikit-learn expects numerical tensors, so we have to convert our `str` data into number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLoC7k4Us5jO"
   },
   "outputs": [],
   "source": [
    "# Strip white-space in 'Region'\n",
    "data_df['Region'] = data_df['Region'].str.strip()\n",
    "\n",
    "# One-hot encoding for 'Region'\n",
    "reg_df = pd.get_dummies(df['Region'], prefix='Region')\n",
    "clean_df = pd.concat([data_df, reg_df], axis=1)\n",
    "clean_df = clean_df.drop(columns=['Region'])\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewBMT3RRhquJ"
   },
   "source": [
    "# 4. Prepare Train/Valid/Test Sets\n",
    "\n",
    "Here you will write code to extract the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2ToBgb8i_ny"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3arWoVGjFkx"
   },
   "source": [
    "Next we split the dataset into training/validation/test set.\n",
    "* Training set: `X_train`, `y_train`\n",
    "* Validation set: `X_valid`, `y_valid`\n",
    "* Test set: `X_test`, `y_test`\n",
    "\n",
    "The following is an example of how to split the dataset into a training and a test sets.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    random_state=42,\n",
    "    test_size=0.20)  # 80:20\n",
    "```\n",
    "\n",
    "Here you will write the code to split `(X, y)` into `(X_train, y_train)`, `(X_valid, y_valid)` and `(X_test, y_test)` using 80/10/10 proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZsvCcKtcxy3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f'Training set: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation set: {X_valid.shape}, {y_valid.shape}')\n",
    "print(f'Test set: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlznPTX0i7Fx"
   },
   "source": [
    "# 5. Model Selection\n",
    "\n",
    "In this section, you will write code to train the model on the training set and evaluate it on the validation set.\n",
    "\n",
    "We will use the simplest ML model for regression problem, which is Linear Regression. This algorithm requires the features to be normalized before training the model.\n",
    "\n",
    "Let's start by create a scaler, which will be used for both the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIVcuIkgyl4P"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Create a scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler with the training set\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale the features in the training set\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "\n",
    "print(f'Mean: {np.mean(scaled_X_train, axis=0)}')\n",
    "print(f'Std: {np.std(scaled_X_train, axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7csc8rhGymnA"
   },
   "source": [
    "Next, let's use the scaled data to train the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igH9f3vzi8q3"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY5qwUQRkCYd"
   },
   "source": [
    "Next, we use the trained model to predict whether the passengers in both the training and the validation sets will survive the titanic crash or not.\n",
    "\n",
    "The predictions for the training and the validation set are stored in `y_hat_train` and `y_hat_valid`.\n",
    "\n",
    "It should be noted that you should reuse the scaler that is **fitted to the training set only**. This is to prevent the scaler from observing unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIuS-PdjkrT9"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjWHE7HskQ7K"
   },
   "source": [
    "Then we determine the prediction performance on the training and the validation set to investigate whether our model has the **underfitting** or **overfitting** problems or not.\n",
    "\n",
    "Here, we use the common metrics for classification problems which are: **accuracy, precision, recall and f1-score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_cpXHeCo7kF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('Training Set')\n",
    "print(f'MSE: {mean_squared_error(y_true=y_train, y_pred=y_hat_train):.4f}')\n",
    "print('')\n",
    "print('Validation Set')\n",
    "print(f'MSE: {mean_squared_error(y_true=y_valid, y_pred=y_hat_valid):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV2uasFIBgF2"
   },
   "source": [
    "**TODO**: Go back to update the parameters of the model to minimize the overfitting and the underfitting as much as you can.\n",
    "\n",
    "Once you are happy with the performance, then we proceed to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTzYCys7oUR7"
   },
   "source": [
    "# 6. Evaluation on Test Set\n",
    "\n",
    "Once we found a best model, we then evaluate the trained model with the test set to estimate the performance on the **unseen** examples.\n",
    "\n",
    "The predictions for the test set are stored in `y_hat_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdinRZgmr1fD"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBvfc5AFr640"
   },
   "outputs": [],
   "source": [
    "print('Test Set')\n",
    "print(f'MSE: {mean_squared_error(y_true=y_test, y_pred=y_hat_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy47pptk6wF8"
   },
   "source": [
    "# 7. Try Other Classifiers\n",
    "\n",
    "There are a large number of supervised-ML algorithms that you can use. Please try other classifiers below and try to achieve the best performance on the test set.\n",
    "\n",
    "* [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso), [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet): try to change `alpha`.\n",
    "* [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html): try to change `n_estimators`, `max_depth`, `min_samples_leaf`.\n",
    "* [Epsilon-Support Vector Regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html): try to change `C`, `gamma`.\n",
    "\n",
    "**TODO**: \n",
    "1. Try the other classifiers as specified above.\n",
    "1. Try other feature scalers mentioned in W6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUlf9U8ur_wq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMk4F6lc1yTlDaNOjG8IqTP",
   "collapsed_sections": [],
   "name": "W6.2 - Model Selection for Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
