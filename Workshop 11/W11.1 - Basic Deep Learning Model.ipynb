{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W11.1 - Basic Deep Learning Model","provenance":[{"file_id":"1KvUMc4NqlF3cLIM6KehmkF_uOLeNFUFQ","timestamp":1603623068531}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HPR_nNQ94KJ-"},"source":["# Prepare Environment"]},{"cell_type":"code","metadata":{"id":"m15_JQeGuaTX"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","from IPython.display import display\n","\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"axes.grid\"] = False\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z--Q9_0x_0GP"},"source":["# TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mQZIcK0Et2F"},"source":["**Note**: most of the code in the notebook is a simplified version of the tutorial example from Tensorflow ([here](https://www.tensorflow.org/tutorials/keras/classification))"]},{"cell_type":"markdown","metadata":{"id":"lXmN9g0ayjnx"},"source":["# Import the Fashion MNIST dataset\n","\n","This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 x 28 pixels), as seen here:\n","\n","<table>\n","  <tr><td>\n","    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n","         alt=\"Fashion MNIST sprite\"  width=\"600\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:"]},{"cell_type":"code","metadata":{"id":"LLM9sJikvkFn"},"source":["from keras.datasets import fashion_mnist\n","\n","# Download Fashion MNIST dataset using `datasets` module in `tf.keras`\n","# Note: the data have already been split into training and test sets\n","(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n","\n","print(f'Training set: {X_train.shape}, {y_train.shape}')\n","print(f'Test set: {X_test.shape}, {y_test.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v11mMcG6TNrA"},"source":["from sklearn.model_selection import train_test_split\n"," \n","X_train, X_valid, y_train, y_valid = train_test_split(\n","    X_train, y_train, \n","    random_state=42,\n","    test_size=10000)\n","\n","print(f'Training set: {X_train.shape}, {y_train.shape}')\n","print(f'Validation set: {X_valid.shape}, {y_valid.shape}')\n","print(f'Test set: {X_test.shape}, {y_test.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cj7twnRj1twi"},"source":["The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n","\n","<table>\n","  <tr>\n","    <th>Label</th>\n","    <th>Class</th>\n","  </tr>\n","  <tr>\n","    <td>0</td>\n","    <td>T-shirt/top</td>\n","  </tr>\n","  <tr>\n","    <td>1</td>\n","    <td>Trouser</td>\n","  </tr>\n","    <tr>\n","    <td>2</td>\n","    <td>Pullover</td>\n","  </tr>\n","    <tr>\n","    <td>3</td>\n","    <td>Dress</td>\n","  </tr>\n","    <tr>\n","    <td>4</td>\n","    <td>Coat</td>\n","  </tr>\n","    <tr>\n","    <td>5</td>\n","    <td>Sandal</td>\n","  </tr>\n","    <tr>\n","    <td>6</td>\n","    <td>Shirt</td>\n","  </tr>\n","    <tr>\n","    <td>7</td>\n","    <td>Sneaker</td>\n","  </tr>\n","    <tr>\n","    <td>8</td>\n","    <td>Bag</td>\n","  </tr>\n","    <tr>\n","    <td>9</td>\n","    <td>Ankle boot</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"VRJChED62ZRw"},"source":["Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"]},{"cell_type":"code","metadata":{"id":"ihj1dsAP1xud"},"source":["class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsyiuNmM2PwN"},"source":["Let's look at an example of the fashion MNIST."]},{"cell_type":"code","metadata":{"id":"mdSF79sb4lIw"},"source":["plt.figure()\n","plt.imshow(X_train[0])\n","plt.colorbar()\n","plt.grid(False)\n","plt.xlabel(class_names[y_train[0]])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qetkkccz5ISk"},"source":["# Data Preprocessing\n","\n","It is a common pratice to **normalize the range of independent variables or features of data**. This is mainly because many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized **so that each feature contributes approximately proportionately to the final distance**.\n","\n","There are many other feature scaling techniques, which can be found in [here](https://en.wikipedia.org/wiki/Feature_scaling).\n","\n","In this example, we'll only scale the inputs to be in the range [0-1] rather than [0-255]."]},{"cell_type":"code","metadata":{"id":"1pnHXIU35HMU"},"source":["# Scale the Fashion MNIST data to be in the range [0-1]\n","# Note: The maximum value of color value is 255\n","X_train = X_train / 255.0\n","X_valid = X_valid / 255.0\n","X_test = X_test / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9MqDWjz-5-IY"},"source":["To verify that the data is in the correct format and that you're ready to build and train the network, let's display the first 25 images from the *training*, *validation* and *test* sets and display the class name below each image."]},{"cell_type":"code","metadata":{"id":"xgrAQgknzX79"},"source":["def plot_data(images, labels):\n","    plt.figure(figsize=(10,10))\n","    for i in range(25):\n","        plt.subplot(5,5,i+1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.grid(False)\n","        plt.imshow(images[i], cmap=plt.cm.binary)\n","        plt.xlabel(class_names[labels[i]])\n","    plt.show()\n","    plt.close('all')\n","\n","print(\"Training set\")\n","plot_data(X_train, y_train)\n","\n","print(\"Validation set\")\n","plot_data(X_valid, y_valid)\n","\n","print(\"Test set\")\n","plot_data(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjZ-uOK07wPM"},"source":["# Define a Model\n","\n","We are going to define a neural network, or what is typically referred to as a deep learning model. Here, we will do a simple 3-layer fully-connected network."]},{"cell_type":"markdown","metadata":{"id":"QPlQ3OIv707z"},"source":["<!--<img src=\"./img/fc_mnist.png\" alt=\"Fully-connected Network\" style=\"width:500px;\"/>-->\n","<img src=\"https://www.dropbox.com/s/6a05qtkgmlih6s4/fc_mnist.png?raw=1\" alt=\"Fully-connected Network\" style=\"width:500px;\"/>"]},{"cell_type":"code","metadata":{"id":"UEzRx5l576Bd"},"source":["from keras.models import Sequential\n","from keras.layers import *\n","\n","num_classes = 10\n","\n","model = keras.Sequential([\n","    # Layer 1 - Flatten the input from an image (28 * 28) to a vector (784)\n","    keras.layers.Flatten(input_shape=(28, 28)),\n","    # Layer 2 - Dense layer (i.e., fully-connected)\n","    keras.layers.Dense(128, activation='relu'),\n","    # Layer 3 - Dense layer (i.e., fully-connected)\n","    keras.layers.Dense(128, activation='relu'),\n","    # Layer 4 - Dense layer (i.e., fully-connected)\n","    # Note: the number of neurons in the last layer must be equal to the number\n","    #       of output classes (which is 10 in this example).\n","    keras.layers.Dense(num_classes, activation='softmax')\n","])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5irwf82Q8xVM"},"source":["The first layer in this network, `tf.keras.layers.Flatten`, transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n","\n","After the pixels are flattened, the network consists of a sequence of two `tf.keras.layers.Dense` layers. These are densely connected, or fully connected, neural layers. The first and second `Dense` layers have 128 nodes (or neurons). The third (or last) layer returns a logits array with length of 10. Each node contains a score that indicates the current image belongs to one of the 10 classes."]},{"cell_type":"markdown","metadata":{"id":"zQCt2mvz8ROs"},"source":["# Train a Model\n","\n","In this section, we will first define several parameters that will be used during the training.\n","\n","*   `epochs`: the number of training epochs (one epoch means the model has seen the entire training samples one times).\n","*   `batch_size`: the number of examples per one training step.\n","*   `learning_rate`: a hyperparameter that defines the adjustment in the weights of our network with respect to the loss gradient.\n"]},{"cell_type":"code","metadata":{"id":"YjNa-rS28XqD"},"source":["epochs = 10\n","batch_size = 256\n","learning_rate = 0.01"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHeeybzy-taV"},"source":["## Loss Function\n","\n","Before we train a model, we need to specify the **loss function**, `loss`, that will be used to quantify the error between the predicted and the target classes. As we would like to train our model to differentiate among 10 fashion classes in the dataset, a loss function that we can use is *cross-entropy*. Cross-entropy is a measure of how different your predicted distribution is from the target distribution (see [Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy) for more details). \n","\n","In this exercise, we will use the cross-entropy.\n","\n","Note: TF-Keras also provides many other loss functions for other problems as well. You can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses)."]},{"cell_type":"code","metadata":{"id":"P8OGmVLq-RNm"},"source":["# Cross-entropy loss\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LvObkjaY-32N"},"source":["## Optimizer\n","\n","Another component that we need to specify before the training is the **optimizer**, `optimizer`. The optimizers that are commonly used to train deep learning models are Stochastic Gradient Descent (SGD), Adam, RMSProp, Adadelta, etc. The list of optimizers provided by TF-Keras can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n","\n","Here we will use SGD."]},{"cell_type":"code","metadata":{"id":"VTq6EcE1-4fo"},"source":["# Stochastic gradient descent (SGD)\n","optimizer = keras.optimizers.SGD(lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzG3YUL8_UdH"},"source":["## Compile the Model\n","\n","Next, we configures the model for training by calling."]},{"cell_type":"code","metadata":{"id":"CmCb0PgA_TJQ"},"source":["model.compile(\n","    loss=loss,\n","    optimizer=optimizer,\n","    metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkB6NPNuW0nE"},"source":["## Train a model\n","\n","We are now ready to train our model. Let's start feeding the data to train the model and it will learn to classify digits.\n","\n","You can read more on the arguments for the `fit` function [here](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit)."]},{"cell_type":"code","metadata":{"id":"LJP7WcsO_bZA"},"source":["hist = model.fit(\n","    X_train, y_train,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_data=(X_valid, y_valid),\n","    # validation_split=0.1,\n","    verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xGeVDPt2sJi"},"source":["fig, ax = plt.subplots(figsize=(8,6))\n","ax.plot(hist.history['loss'], label='train')\n","ax.plot(hist.history['val_loss'], label='valid')\n","ax.set_ylabel('Loss')\n","ax.set_xlabel('Epochs')\n","plt.legend()\n","plt.show()\n","\n","fig, ax = plt.subplots(figsize=(8,6))\n","ax.plot(hist.history['accuracy'], label='train')\n","ax.plot(hist.history['val_accuracy'], label='valid')\n","ax.set_ylabel('Accuracy')\n","ax.set_xlabel('Epochs')\n","plt.legend()\n","plt.show()\n","\n","plt.close('all')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Z5vkzZdQ6w0"},"source":["Let's see the model prediction in details. Here we will apply the trained model on the validation set."]},{"cell_type":"code","metadata":{"id":"shG_yjN8Qsdy"},"source":["# Predict the labels of these images\n","y_hat_valid_probs = model.predict(X_valid)\n","\n","print(y_hat_valid_probs.shape)\n","print(y_hat_valid_probs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Bmbn6pcREEG"},"source":["It can be seen that the outputs from the `predict` function are the probability distribution of each class. Typically, we will select the class with the highest probabiliy as the predicted class for each input image."]},{"cell_type":"code","metadata":{"id":"xwch9zW7RSYl"},"source":["# Convert the label back to the original format\n","y_hat_valid = np.argmax(y_hat_valid_probs, axis=-1)\n","\n","print(y_hat_valid.shape)\n","print(y_hat_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZ66Xjd_SSnM"},"source":["To make it more human-friendly, we will visualize the input image and its corresponding prediction to see how our model performs."]},{"cell_type":"code","metadata":{"id":"6uLIb4CiDEn4"},"source":["def plot_image(i, probs, true_label, img):\n","    probs, true_label, img = probs, true_label[i], img[i]\n","    plt.grid(False)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.imshow(img, cmap=plt.cm.binary)\n","    predicted_label = np.argmax(probs)\n","    if predicted_label == true_label:\n","        color = 'blue'\n","    else:\n","        color = 'red'\n","    plt.xlabel(\n","        '{} {:2.0f}% ({})'.format(\n","            class_names[predicted_label],\n","            100*np.max(probs),\n","            class_names[true_label]),\n","        color=color)\n","    \n","def plot_prob_dist(i, probs, true_label):\n","    probs, true_label = probs, true_label[i]\n","    plt.grid(False)\n","    plt.xticks(range(10))\n","    plt.yticks([])\n","    thisplot = plt.bar(range(10), probs, color=\"#777777\")\n","    plt.ylim([0, 1])\n","    predicted_label = np.argmax(probs, axis=-1)\n","    thisplot[predicted_label].set_color('red')\n","    thisplot[true_label].set_color('blue')\n","\n","def plot_output(probs, images, labels):\n","    num_rows = 5\n","    num_cols = 3\n","    num_images = num_rows*num_cols\n","    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n","    for i in range(num_images):\n","        plt.subplot(num_rows, 2*num_cols, 2*i+1)\n","        plot_image(i, probs[i], labels, images)\n","        plt.subplot(num_rows, 2*num_cols, 2*i+2)\n","        plot_prob_dist(i, probs[i], labels)\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrICW-6NRvxx"},"source":["<table>\n","  <tr>\n","    <th>Label</th>\n","    <th>Class</th>\n","  </tr>\n","  <tr>\n","    <td>0</td>\n","    <td>T-shirt/top</td>\n","  </tr>\n","  <tr>\n","    <td>1</td>\n","    <td>Trouser</td>\n","  </tr>\n","    <tr>\n","    <td>2</td>\n","    <td>Pullover</td>\n","  </tr>\n","    <tr>\n","    <td>3</td>\n","    <td>Dress</td>\n","  </tr>\n","    <tr>\n","    <td>4</td>\n","    <td>Coat</td>\n","  </tr>\n","    <tr>\n","    <td>5</td>\n","    <td>Sandal</td>\n","  </tr>\n","    <tr>\n","    <td>6</td>\n","    <td>Shirt</td>\n","  </tr>\n","    <tr>\n","    <td>7</td>\n","    <td>Sneaker</td>\n","  </tr>\n","    <tr>\n","    <td>8</td>\n","    <td>Bag</td>\n","  </tr>\n","    <tr>\n","    <td>9</td>\n","    <td>Ankle boot</td>\n","  </tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"MUsLDGc-C4a8"},"source":["# Color correct predictions in blue and incorrect predictions in red.\n","plot_output(y_hat_valid_probs, X_valid, y_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SP4PPu_eScBY"},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","print('Validation Set')\n","print(confusion_matrix(y_true=y_valid, y_pred=y_hat_valid))\n","print(f'Accuracy: {accuracy_score(y_true=y_valid, y_pred=y_hat_valid):.2f}')\n","print(f'Macro F1-score: {f1_score(y_true=y_valid, y_pred=y_hat_valid, average=\"macro\"):.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uukGYk3JXIgP"},"source":["# Evaluate Performance on Test Set\n","\n","Once you have finished the model training, you then evaluate the classification performance on the test set (i.e., the unseen dataset)."]},{"cell_type":"code","metadata":{"id":"TIlI5WiVjd4h"},"source":["# Predict the labels of these images\n","y_hat_test_probs = model.predict(X_test)\n","\n","print(y_hat_test_probs.shape)\n","print(y_hat_test_probs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"69hgt_AxAt1E"},"source":["# Convert the label back to the original format\n","y_hat_test = np.argmax(y_hat_test_probs, axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXZLfaurAblZ"},"source":["# Output\n","print('Test Set')\n","print(confusion_matrix(y_true=y_test, y_pred=y_hat_test))\n","print(f'Accuracy: {accuracy_score(y_true=y_test, y_pred=y_hat_test):.2f}')\n","print(f'Macro F1-score: {f1_score(y_true=y_test, y_pred=y_hat_test, average=\"macro\"):.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KRSdi9DkfOk"},"source":["# Error Analysis\n","\n","It's always a good idea to inspect the output and make sure everything looks fine. Here we'll look at some examples our model gets right, and some examples it gets wrong on the test sets.\n","\n","First, we determine which samples are correct or incorrect on the test set."]},{"cell_type":"code","metadata":{"id":"rtlKYzaVkq__"},"source":["correct_indices = np.where(y_hat_test == y_test)[0]\n","incorrect_indices = np.where(y_hat_test != y_test)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKLdFq6YkrgF"},"source":["Then we plot the images with their corresponding classes. In the incorrect case, we also plot the ground truth classes for comparison."]},{"cell_type":"code","metadata":{"id":"5EVBgXTbUvjn"},"source":["# Correct\n","idx = np.random.choice(np.arange(len(correct_indices)), 15)\n","print('Correct')\n","plot_output(\n","    y_hat_test_probs[correct_indices[idx]],\n","    X_test[correct_indices[idx]],\n","    y_test[correct_indices[idx]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUe20C0MVKKS"},"source":["# Incorrect\n","idx = np.random.choice(np.arange(len(incorrect_indices)), 15)\n","print('Incorrect')\n","plot_output(\n","    y_hat_test_probs[incorrect_indices[idx]],\n","    X_test[incorrect_indices[idx]],\n","    y_test[incorrect_indices[idx]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5JXyu9BxlUSO"},"source":["# Play around\n","\n","Now it is your turn! Let's try to change the model architecture and the optimizer to see the effects.\n","\n","For example,\n","* Change the number of fully-connected layers\n","    * e.g., 2, 3, 4 layers\n","* Change the number of hidden units\n","    * e.g., 10, 128, 256, 512\n","* Change the optimizers (i.e., `optimizer`)\n","    * e.g., [keras.optimizers.RMSprop](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop), [keras.optimizers.Adadelta](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta), [keras.optimizers.Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n","* Change the learning rate of the optimizer (i.e., `learning_rate`)\n","    * e.g., 10000, 0.00001, 0.001\n","* Change the number of training epochs (i.e., `epochs`)\n","    * e.g., 1, 10, 20"]},{"cell_type":"code","metadata":{"id":"-P9dZSYrlRL-"},"source":[""],"execution_count":null,"outputs":[]}]}